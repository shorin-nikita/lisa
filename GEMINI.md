# L.I.S.A. (Л.И.С.А.) - Local Intelligent Automation System

## Project Overview

**L.I.S.A.** is a self-hosted AI platform designed to deploy a local AI environment quickly. It integrates powerful open-source tools into a unified Docker Compose stack.

**Key Components:**
*   **N8N:** Workflow automation (custom build with FFmpeg for media processing).
*   **Supabase:** Open source Firebase alternative (PostgreSQL, Authentication, Realtime).
*   **Ollama:** Local LLM runner (supports Llama, Mistral, Gemma, etc.).
*   **Whisper:** OpenAI-compatible speech-to-text service.
*   **Qdrant:** Vector database for RAG (Retrieval-Augmented Generation).
*   **Caddy:** Reverse proxy with automatic HTTPS management.
*   **Squid:** Optional forward proxy for routing external API requests (e.g., to OpenAI/Anthropic).

## Architecture

The project uses a **Multi-Compose Stack Pattern** consolidated under a single Docker project named `localai`.

*   **Main Config:** `docker-compose.yml` (AI services: N8N, Ollama, Whisper, etc.)
*   **Supabase Config:** `supabase/docker/docker-compose.yml` (included via `include` directive).
*   **Network:** All services share the `localai_default` network.
*   **Profiles:**
    *   `cpu`: Standard CPU-only mode (default).
    *   `gpu-nvidia`: Enables NVIDIA GPU acceleration for Ollama.
    *   `gpu-amd`: Enables AMD GPU acceleration (ROCm) for Ollama.
    *   `proxy`: Enables Squid proxy if configured.

### Key Directories
*   `n8n/`: N8N configuration and backups.
    *   `n8n/backup/workflows`: Pre-configured workflows imported on first run.
*   `n8n-ffmpeg/`: Custom Dockerfile for N8N with FFmpeg.
*   `supabase/`: Supabase configuration (sparse checkout of `docker/` folder).
*   `shared/`: Shared directory (chmod 777) for file exchange between containers.
*   `squid/`: Proxy configuration templates.

## Management Commands

### Installation & Setup
Run the interactive setup script to detect hardware, generate secrets, and configure the environment:
```bash
python3 CTAPT.py
```

### Updates
Update the system, pull new images, and restart services:
```bash
python3 O6HOBA.py
```

### Service Control
**Start Services:**
```bash
# Default (public environment, auto-detected profile)
python3 start_services.py

# specific profile/environment
python3 start_services.py --profile cpu --environment private
```

**Docker Compose (Manual):**
*ALWAYS* use `-p localai` to target the correct project.
```bash
# Stop all services
docker compose -p localai down

# View logs
docker compose -p localai logs -f [service_name]

# Rebuild specific service (e.g., n8n)
docker compose -p localai build n8n
docker compose -p localai up -d n8n
```

## Configuration

### Environment Variables
The `.env` file is automatically generated by `CTAPT.py`. Key sections:
*   **Secrets:** `N8N_ENCRYPTION_KEY`, `POSTGRES_PASSWORD`, `JWT_SECRET`, etc.
*   **Domains:** `N8N_HOSTNAME`, `SUPABASE_HOSTNAME`.
*   **Resources:** `{SERVICE}_CPU_LIMIT`, `{SERVICE}_MEM_LIMIT` (calculated by update script).
*   **Proxy:** `PROXY_ENABLED`, `PROXY_IP`, `PROXY_PORT` (for Squid).

### Task Runners (External Mode)
N8N runs with **External Task Runners** for security and stability (Python/JS code nodes run in a separate container).
*   **Broker:** `n8n:5679`
*   **Runners:** `n8n-runners` container
*   **Auth:** `N8N_RUNNERS_AUTH_TOKEN` (auto-generated)

## Access & Usage

| Service | Internal URL (Docker) | External URL (Localhost) | External URL (Domain) |
|---------|-----------------------|--------------------------|-----------------------|
| **N8N** | `http://n8n:5678` | `http://localhost:8001` | `https://{N8N_HOSTNAME}` |
| **Supabase** | `http://kong:8000` | `http://localhost:8005` | `https://{SUPABASE_HOSTNAME}` |
| **Ollama** | `http://ollama:11434` | - | - |
| **Whisper** | `http://whisper:8000` | - | - |
| **Qdrant** | `http://qdrant:6333` | - | - |

## Troubleshooting

*   **FFmpeg Missing:** If N8N says `ffmpeg: not found`, rebuild the image: `docker compose -p localai build n8n`.
*   **Postgres Unhealthy:** Check `docker logs localai-postgres-1`. Ensure `POSTGRES_VERSION` in `.env` matches the data volume version.
*   **Permission Denied (Shared):** Ensure `shared/` directory has `777` permissions.
*   **Ollama Models:** Models like `gemma3:1b` are auto-downloaded by the `ollama-pull-models` init container.
