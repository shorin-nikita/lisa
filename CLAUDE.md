# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

# Л.И.С.А. — Локальная Интеллектуальная Система Автоматизации

Self-hosted AI platform combining N8N automation, Supabase database, Ollama LLMs, Open WebUI, Whisper speech recognition, and media processing in a unified Docker Compose stack.

## Core Architecture

### Multi-Compose Stack Pattern

The project uses a **unified Docker Compose architecture** with two separate compose files merged under one project:

1. **Main stack** (`docker-compose.yml`) — AI services (N8N, Ollama, Whisper, Qdrant, etc.)
2. **Supabase stack** (`supabase/docker/docker-compose.yml`) — Full Supabase suite included via `include:` directive
3. **All services share**:
   - Single Docker network: `localai_default`
   - Single project name: `localai`
   - Single `.env` file (copied to `supabase/docker/.env` by startup script)

**Critical**: Never use separate project names. All `docker compose` commands MUST use `-p localai` to maintain service connectivity.

### GPU Profile System

Ollama uses Docker Compose profiles to support different hardware:

- `cpu` → `ollama-cpu` service (no GPU acceleration)
- `gpu-nvidia` → `ollama-gpu` service with NVIDIA device reservations
- `gpu-amd` → `ollama-gpu-amd` service with ROCm image and `/dev/kfd`, `/dev/dri` devices

Each profile has a matching init container (`ollama-pull-vikhr`) that auto-downloads `lakomoor/vikhr-llama-3.2-1b-instruct` (компактная модель для русского языка, <3GB) and `nomic-embed-text` models on first run.

### Custom N8N Build

N8N uses a **custom Dockerfile** (`n8n-ffmpeg/Dockerfile`) that adds FFmpeg to the official image:
- Base: `n8nio/n8n:latest`
- Adds: FFmpeg + ffprobe via `apk add`
- Purpose: Enable media processing in N8N Execute Command nodes

**Important**: This image must be rebuilt when N8N updates. Use `docker compose -p localai build n8n` to rebuild.

## Development Commands

### Installation

```bash
# First-time setup (interactive)
python3 CTAPT.py
```

This script:
1. Detects GPU (NVIDIA/AMD/CPU)
2. Validates domains and email for Let's Encrypt
3. Generates all secrets (N8N encryption keys, Supabase JWT, database passwords)
4. Creates `.env` file with configuration
5. Configures UFW firewall (ports 22, 80, 443) — **CRITICAL**: SSH rule added BEFORE enabling firewall
6. Calls `start_services.py` to launch stack

### Update System

```bash
# Update to latest version
python3 O6HOBA.py
```

Update process:
1. Detect current GPU profile and environment from `.env`
2. Create timestamped backup tarball (`.env`, `n8n/backup`, `neo4j/data`, `shared/`)
3. Stop all services with `docker compose -p localai down`
4. Pull Git updates from `main` branch
5. Calculate and update resource limits in `.env` based on detected CPU/RAM
6. Pull new Docker images with `docker compose pull --ignore-buildable`
7. Rebuild custom images (`n8n-ffmpeg`)
8. Restart services via `start_services.py`

### Manual Service Control

```bash
# Start with specific profile and environment
python3 start_services.py --profile cpu --environment public

# Options:
#   --profile: cpu | gpu-nvidia | gpu-amd | none
#   --environment: private | public
```

**Startup sequence** (`start_services.py`):
1. Validate `.env` has required variables (`POSTGRES_PASSWORD`, `N8N_ENCRYPTION_KEY`, `JWT_SECRET`)
2. Clone Supabase repo with sparse checkout (only `docker/` directory) if missing
3. Create `shared/` directory with mode 777
4. Copy `.env` → `supabase/docker/.env`
5. Stop existing containers: `docker compose -p localai down`
6. Start Supabase stack: `docker compose -p localai -f supabase/docker/docker-compose.yml up -d`
7. Wait 30 seconds for Supabase initialization
8. Start AI stack: `docker compose -p localai --profile <profile> -f docker-compose.yml [-f docker-compose.override.<env>.yml] up -d`
9. Wait for PostgreSQL healthcheck to become `healthy` (max 120s)

### Docker Compose Commands

```bash
# View all running containers
docker ps

# Logs for entire project
docker compose -p localai logs -f

# Logs for specific service
docker logs n8n -f
docker logs whisper -f
docker logs ollama -f

# Stop all services (preserves volumes/data)
docker compose -p localai down

# Stop and DELETE all data (destructive!)
docker compose -p localai down -v

# Rebuild specific service
docker compose -p localai build n8n
docker compose -p localai up -d n8n

# Resource monitoring
docker stats

# Important: ALWAYS use -p localai to target the correct project
```

## Implementation Details

### Environment Configuration

The `.env` file is generated by `CTAPT.py` and contains:

**Generated secrets** (via `secrets` module):
- `N8N_ENCRYPTION_KEY`, `N8N_USER_MANAGEMENT_JWT_SECRET` (32-byte hex)
- `POSTGRES_PASSWORD`, `DASHBOARD_PASSWORD` (alphanumeric only, no special chars to avoid URL encoding issues)
- `SECRET_KEY_BASE`, `VAULT_ENC_KEY` (for Supabase)
- `LOGFLARE_PUBLIC_ACCESS_TOKEN`, `LOGFLARE_PRIVATE_ACCESS_TOKEN`

**User-provided values**:
- `JWT_SECRET`, `ANON_KEY`, `SERVICE_ROLE_KEY` — Supabase auth keys (generate at https://supabase.com/docs/guides/self-hosting/docker#generate-api-keys)
- `N8N_HOSTNAME`, `WEBUI_HOSTNAME`, `SUPABASE_HOSTNAME` — Domains for HTTPS access
- `LETSENCRYPT_EMAIL` — **Must be real email**, Let's Encrypt rejects fake addresses like `test@test.test`

**Resource limits** (auto-calculated by `O6HOBA.py`):
- Format: `{SERVICE}_CPU_LIMIT`, `{SERVICE}_MEM_LIMIT`, `{SERVICE}_CPU_RESERVE`, `{SERVICE}_MEM_RESERVE`
- Default allocation: Ollama 40%, PostgreSQL 20%, N8N 15%, Qdrant/WebUI 10%

### Supabase Sparse Checkout

Supabase is cloned with Git sparse checkout to download only the `docker/` directory (~200MB instead of 2GB):

```bash
git clone --filter=blob:none --no-checkout https://github.com/supabase/supabase.git
cd supabase
git sparse-checkout init --cone
git sparse-checkout set docker
git checkout master
```

The main compose file includes it via:
```yaml
include:
  - ./supabase/docker/docker-compose.yml
```

### N8N Auto-Import Pattern

N8N workflows/credentials are imported on first launch using a **one-shot init container**:

```yaml
n8n-import:
  container_name: n8n-import
  entrypoint: /bin/sh
  command:
    - "-c"
    - "n8n import:credentials --separate --input=/backup/credentials && n8n import:workflow --separate --input=/backup/workflows"
  volumes:
    - ./n8n/backup:/backup

n8n:
  depends_on:
    n8n-import:
      condition: service_completed_successfully
```

The main N8N container waits for successful import completion before starting.

### Shared Directory

The `shared/` directory enables file exchange between containers:
- N8N mounts it as `/data/shared`
- Used for temporary media files during FFmpeg processing
- **Must have mode 777** for write access from all containers
- Auto-created by `CTAPT.py` and `start_services.py`

### Caddy Dual-Mode Routing

Caddy supports **simultaneous domain and localhost access** via environment variable syntax:

```caddyfile
{$N8N_HOSTNAME}, :8001 {
    reverse_proxy n8n:5678
}
```

- If `N8N_HOSTNAME=n8n.example.com` → HTTPS with Let's Encrypt cert
- If `N8N_HOSTNAME=:8001` → HTTP on localhost:8001
- Both modes can coexist by listing both (domain AND port)

### Service Internal URLs

**Whisper** (OpenAI-compatible speech-to-text):
- URL: `http://whisper:8000`
- Endpoint: `POST /v1/audio/transcriptions`
- Body: `multipart/form-data` with `file` (binary) and `model` (`tiny`/`base`/`small`)
- Recommended: `base` model for CPU (best speed/accuracy balance)

**Other key services**:
- Ollama: `http://ollama:11434`
- Qdrant: `http://qdrant:6333`
- Supabase API: `http://kong:8000` (via Kong gateway)
- PostgreSQL (N8N): `postgres:5432`
- PostgreSQL (Supabase): `db:5432`

### PostgreSQL Healthcheck Wait

`start_services.py` waits for PostgreSQL healthcheck before completing:

```python
def wait_for_postgres_healthy(timeout=120):
    result = subprocess.run(
        ["docker", "inspect", "--format", "{{.State.Health.Status}}", "localai-postgres-1"],
        capture_output=True, text=True
    )
    return result.stdout.strip() == "healthy"
```

This prevents N8N connection failures during parallel startup.

## Pre-built N8N Workflows

Located in `n8n/backup/workflows/`, auto-imported on first launch:

- **Telegram Bot.json** — Full-featured bot with voice transcription, image analysis, analytics commands
- **HTTP Handle.json** — Webhook configuration helper
- **Конвертация WebM → OGG + Транскрибация.json** — Media conversion with Whisper transcription
- **V1_Local_RAG_AI_Agent.json** — Basic RAG with Ollama
- **V2_Local_Supabase_RAG_AI_Agent.json** — RAG with Supabase vector store (pgvector)
- **V3_Local_Agentic_RAG_AI_Agent.json** — Advanced agentic RAG

## Common Issues

### FFmpeg not found in N8N

**Problem**: `ffmpeg: not found` in Execute Command node

**Solution**: Rebuild the custom N8N image:
```bash
docker inspect n8n | grep n8n-ffmpeg  # Verify custom image is in use
docker compose -p localai build n8n
docker compose -p localai up -d n8n
```

### PostgreSQL unhealthy

**Problem**: `container localai-postgres-1 is unhealthy`

**Solution**:
```bash
docker logs localai-postgres-1  # Check logs
grep POSTGRES_VERSION .env      # Verify version is 16
docker compose -p localai down -v && python3 CTAPT.py  # Nuclear option: wipe and reinstall
```

### Permission denied in shared/

**Problem**: N8N cannot write to `/data/shared`

**Solution**: `chmod 777 shared/` (auto-fixed by `start_services.py`)

### HTTPS not working

**Problem**: `ERR_SSL_PROTOCOL_ERROR` on domain access

**Checklist**:
1. `.env` has real email (not `test@test.test`)
2. DNS A-records point to server IP
3. Ports 80/443 open in firewall
4. Check Caddy logs: `docker logs caddy -f`

### Whisper crashes on large files

**Problem**: `socket hang up` or `ECONNRESET` during transcription

**Solutions**:
- Use `base` model instead of `medium`/`large` on CPU
- Split files into <25MB chunks
- Increase container memory limits

## Access Ports

**External (via Caddy)**:
- N8N: http://localhost:8001 or https://{N8N_HOSTNAME}
- Open WebUI: http://localhost:8002 or https://{WEBUI_HOSTNAME}
- Supabase: http://localhost:8005 or https://{SUPABASE_HOSTNAME}

**Internal (Docker network)**:
- N8N: `n8n:5678`
- Ollama: `ollama:11434`
- Whisper: `whisper:8000`
- Qdrant: `qdrant:6333`
- PostgreSQL (N8N): `postgres:5432`
- PostgreSQL (Supabase): `db:5432`
- Kong (Supabase API): `kong:8000`

## Key Architectural Patterns

1. **Init Container Pattern**: Used for N8N workflow import and Ollama model downloads
2. **Sparse Git Checkout**: Supabase repo cloned with only `docker/` directory to save bandwidth
3. **Healthcheck Dependencies**: PostgreSQL healthcheck prevents race conditions during startup
4. **Dual-Environment Support**: Same compose files work for private (localhost) and public (domain) deployments
5. **Resource Limits**: Auto-calculated from system specs, stored in `.env`, applied via Docker deploy resources
6. **Custom Image Builds**: N8N image extended with FFmpeg, requires manual rebuild on upstream updates
